# =============================
# Amicus Brief â€” Top-10 Key Items (GPU-ready Colab Notebook)
# =============================

# -----------------------------
# 1. Install dependencies
# -----------------------------
!pip install -q pymupdf sentence-transformers transformers scikit-learn networkx python-pptx nltk pytesseract pillow
!apt-get update -qq && apt-get install -y -qq tesseract-ocr

# -----------------------------
# 2. Imports and setup
# -----------------------------
import os, json
import fitz  # PyMuPDF
import nltk
nltk.download('punkt')  # sentence tokenizer
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from sklearn.cluster import AgglomerativeClustering
from transformers import pipeline
from pptx import Presentation
from pptx.util import Inches
from PIL import Image
import pytesseract
import torch

# Detect GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# -----------------------------
# 3. Upload or mount PDF
# -----------------------------
from google.colab import drive
drive.mount('/content/drive')
pdf_path = "/content/drive/MyDrive/Amicus Brief on Behalf of Mississippi, Alabama, Alaska, Arkansas etc....pdf"

# -----------------------------
# 4. PDF extraction (text + OCR fallback)
# -----------------------------
def extract_page_text_fitx(pdf_path):
    doc = fitz.open(pdf_path)
    pages = []
    for pno in range(doc.page_count):
        page = doc[pno]
        text = page.get_text("text")
        pages.append({"page_num": pno+1, "raw_text": text})
    doc.close()
    return pages

def ocr_page_image(pix):
    mode = "RGB" if pix.n < 4 else "RGBA"
    img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)
    return pytesseract.image_to_string(img)

def ensure_text_for_pages(pdf_path, pages):
    doc = fitz.open(pdf_path)
    for p in pages:
        if not p["raw_text"].strip():
            pno = p["page_num"] - 1
            pix = doc[pno].get_pixmap(dpi=200)
            try:
                p["raw_text"] = ocr_page_image(pix)
            except:
                p["raw_text"] = ""
    doc.close()
    return pages

print("Extracting pages...")
pages = extract_page_text_fitx(pdf_path)
pages = ensure_text_for_pages(pdf_path, pages)
print(f"Extracted {len(pages)} pages.")

# -----------------------------
# 5. Build sentence index
# -----------------------------
def build_page_lines(pages):
    for p in pages:
        p["lines"] = p["raw_text"].splitlines()
    return pages

def build_sentence_index(pages):
    sent_index = []
    for p in pages:
        text = p["raw_text"].strip()
        if not text: continue
        sents = sent_tokenize(text)
        running = 0
        flat_text = text
        for s in sents:
            idx = flat_text.find(s, running)
            if idx == -1: idx = running
            line_start, line_end = None, None
            cumul = 0
            for i, line in enumerate(p["lines"]):
                line_len = len(line)+1
                if line_start is None and cumul <= idx < cumul+line_len:
                    line_start = i+1
                if cumul <= idx+len(s) <= cumul+line_len:
                    line_end = i+1
                    break
                cumul += line_len
            if line_start is None: line_start = 1
            if line_end is None: line_end = line_start
            sent_index.append({
                "sentence": s.strip(),
                "page": p["page_num"],
                "line_start": line_start,
                "line_end": line_end
            })
            running = idx + len(s)
    return sent_index

pages = build_page_lines(pages)
sent_index = build_sentence_index(pages)
print(f"Total sentences: {len(sent_index)}")

# -----------------------------
# 6. Compute embeddings + TextRank
# -----------------------------
print("Loading SentenceTransformer...")
embed_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
sentences = [d["sentence"] for d in sent_index]

# Batched embedding for GPU memory efficiency
batch_size = 64
embs = []
for i in range(0, len(sentences), batch_size):
    batch_emb = embed_model.encode(sentences[i:i+batch_size], convert_to_numpy=True)
    embs.append(batch_emb)
embs = np.vstack(embs)

def textrank_scores_from_embeddings(embeddings, threshold=0.25):
    sim = cosine_similarity(embeddings)
    n = len(embeddings)
    G = nx.Graph()
    for i in range(n):
        for j in range(i+1, n):
            w = float(sim[i,j])
            if w > threshold:
                G.add_edge(i,j,weight=w)
    if len(G.edges)==0:
        return np.ones(n)/n
    pr = nx.pagerank(G, weight='weight')
    scores = np.array([pr.get(i,0.0) for i in range(n)])
    return scores / scores.sum() if scores.sum()>0 else scores

print("Computing TextRank scores...")
textrank_scores = textrank_scores_from_embeddings(embs, threshold=0.25)

# -----------------------------
# 7. Stance detection (GPU)
# -----------------------------
print("Loading zero-shot classifier...")
zero_shot = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=0 if torch.cuda.is_available() else -1
)
candidate_labels = ["supports the brief's main claim", "opposes the brief's main claim", "neutral factual statement"]

def get_stance_for_sentences(sentences, batch_size=32):
    stances = []
    for i in range(0,len(sentences),batch_size):
        chunk = sentences[i:i+batch_size]
        out = zero_shot(chunk, candidate_labels, hypothesis_template="This sentence {}.")
        if isinstance(out, dict): out = [out]
        for r in out:
            label = r['labels'][0]
            if "supports" in label: stance = "for"
            elif "opposes" in label: stance = "against"
            else: stance = "neutral"
            stances.append({"label":label, "stance":stance, "score":float(r['scores'][0])})
    return stances

print("Classifying stance...")
stances = get_stance_for_sentences(sentences, batch_size=32)

# -----------------------------
# 8. Cluster sentences into candidates
# -----------------------------
def cluster_sentences(embeddings, approx_clusters=None):
    n = embeddings.shape[0]
    if approx_clusters is None: approx_clusters = max(5, n//15)
    clustering = AgglomerativeClustering(
        n_clusters=min(approx_clusters,n),
        affinity='cosine', linkage='average'
    )
    labels = clustering.fit_predict(embeddings)
    groups = {}
    for i,lbl in enumerate(labels):
        groups.setdefault(lbl,[]).append(i)
    return list(groups.values())

groups = cluster_sentences(embs)

# -----------------------------
# 9. Aggregate candidates
# -----------------------------
def aggregate_and_score(groups, sent_index, textrank_scores, stances):
    candidates = []
    for g in groups:
        group_score = float(sum([textrank_scores[i] for i in g]))
        s_counts = {"for":0,"against":0,"neutral":0}
        for i in g:
            s_counts[stances[i]['stance']] +=1
        rep_idx = max(g, key=lambda i: textrank_scores[i])
        rep = sent_index[rep_idx]
        candidate = {
            "indices": g,
            "score": group_score,
            "stance_counts": s_counts,
            "rep_index": rep_idx,
            "rep_sentence": rep["sentence"],
            "pages_lines": [(sent_index[i]["page"], sent_index[i]["line_start"], sent_index[i]["line_end"]) for i in g],
            "sentences": [sent_index[i]["sentence"] for i in g]
        }
        candidates.append(candidate)
    return sorted(candidates, key=lambda x:x["score"], reverse=True)

candidates = aggregate_and_score(groups, sent_index, textrank_scores, stances)

# -----------------------------
# 10. Summarize candidates and Top-10 (GPU)
# -----------------------------
print("Loading summarizer...")
summarizer = pipeline(
    "summarization",
    model="sshleifer/distilbart-cnn-12-6",
    device=0 if torch.cuda.is_available() else -1
)

def summarize_text(text,max_length=60,min_length=15):
    try:
        out=summarizer(text,max_length=max_length,min_length=min_length,truncation=True)
        return out[0]['summary_text']
    except:
        return (text[:200]+"...").strip()

def produce_topk(candidates,k=10):
    topk=[]
    for cand in candidates[:k]:
        idxs_sorted = sorted(cand["indices"], key=lambda i: -textrank_scores[i])
        top_idxs = idxs_sorted[:3]
        text_blob = " ".join([sent_index[i]["sentence"] for i in top_idxs])[:1000]  # safe slice
        summary = summarize_text(text_blob)
        rep = sent_index[cand["rep_index"]]
        sc = cand["stance_counts"]
        if sc["for"]>sc["against"]: majority_stance="for"
        elif sc["against"]>sc["for"]: majority_stance="against"
        else: majority_stance="mixed/neutral"
        topk.append({
            "summary": summary,
            "excerpt": cand["rep_sentence"],
            "page": rep["page"],
            "line_start": rep["line_start"],
            "line_end": rep["line_end"],
            "score": float(cand["score"]),
            "stance": majority_stance,
            "evidence_sentences": cand["sentences"],
            "pages_lines": cand["pages_lines"]
        })
    return topk

top10 = produce_topk(candidates,k=10)

for i,t in enumerate(top10,1):
    print(f"\n--- Item {i} (stance:{t['stance']}) Page {t['page']} Lines {t['line_start']}-{t['line_end']} ---")
    print("Summary:", t['summary'])
    print("Excerpt:", t['excerpt'][:300])

# -----------------------------
# 11. Export JSON & PPTX
# -----------------------------
def save_json(results,path="top10.json"):
    with open(path,"w",encoding="utf-8") as f:
        json.dump(results,f,ensure_ascii=False,indent=2)

def write_pptx(results,out_ppt="brief_key_items.pptx"):
    prs = Presentation()
    for i,item in enumerate(results,1):
        slide = prs.slides.add_slide(prs.slide_layouts[1])  # Title + Content
        slide.shapes.title.text=f"Key Item #{i} â€” Page {item['page']} Lines {item['line_start']}-{item['line_end']}"
        txBox = slide.placeholders[1]
        txBox.text = (
            f"Summary: {item['summary']}\n\n"
            f"Excerpt: {item['excerpt']}\n\n"
            f"Stance: {item['stance']}\n"
            f"Score: {item['score']:.4f}"
        )
    prs.save(out_ppt)

save_json(top10,"top10.json")
write_pptx(top10,"brief_key_items.pptx")
print("Saved top10.json and brief_key_items.pptx.")

# -----------------------------
# 12. Download outputs
# -----------------------------
from google.colab import files
files.download("top10.json")
files.download("brief_key_items.pptx")
